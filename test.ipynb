{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "sampled_images = torch.rand(252,6,256,256,3)\n",
    "subject = torch.cat([i*torch.ones(50) for i in range(4)] + [4*torch.ones(52)])\n",
    "print(subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_label = torch.cat([torch.cat(5*[torch.arange(150,200)]),torch.tensor([198]),torch.tensor([199])])\n",
    "print(naive_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def concat_images(sampled_images, subject, naive_label):\n",
    "    # all_sampled_images = torch.stack(sampled_images)  # (250+,num_samples+1,3,256,256)\n",
    "    # all_subject_idx = torch.stack(subject)  # (250+,)\n",
    "    # all_naive_label = torch.stack(naive_label)  # (250+,)\n",
    "    all_sampled_images = sampled_images\n",
    "    all_subject_idx = subject\n",
    "    all_naive_label = naive_label\n",
    "    # 按照subject_idx取samples\n",
    "    num_subjects = 5\n",
    "    \n",
    "    all_sorted_unique_images = []\n",
    "\n",
    "    for i in range(num_subjects):\n",
    "        selected_images = all_sampled_images[all_subject_idx == i]\n",
    "        selected_naive_labels = all_naive_label[all_subject_idx == i]\n",
    "\n",
    "        sorted_naive_labels, indices= torch.sort(selected_naive_labels)\n",
    "        sorted_images = selected_images[indices]\n",
    "        # sorted_images = torch.gather(selected_images,0,indices)\n",
    "\n",
    "        unique_sorted_naive_labels, unique_indices = np.unique(sorted_naive_labels, return_index=True)\n",
    "        xx = sorted_naive_labels[unique_indices] # 应该与unique_sorted_naive_label一致\n",
    "        unique_sorted_images = sorted_images[unique_indices]\n",
    "\n",
    "        # unique_naive_labels, inverse_indices = torch.unique(selected_naive_labels, return_inverse=True)\n",
    "        # unique_images = selected_images[inverse_indices]\n",
    "\n",
    "        # sorted_unique_naive_labels, indices= torch.sort(unique_naive_labels)\n",
    "        # sorted_unique_images = unique_images[indices]\n",
    "\n",
    "\n",
    "        all_sorted_unique_images.append(unique_sorted_images)\n",
    "    results = torch.stack(all_sorted_unique_images)\n",
    "    return results\n",
    "\n",
    "\n",
    "aa = concat_images(sampled_images, subject, naive_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_shape = 1024\n",
    "(normalized_shape,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_shape = (normalized_shape,)\n",
    "a = tuple(normalized_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\U0001F383'*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "c = torch.rand(5,291,1024)\n",
    "norm = nn.LayerNorm(1024)\n",
    "out = norm(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(norm.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.randn(1, 2, 3, 4)\n",
    "a.size()\n",
    "# torch.Size([1, 2, 3, 4])\n",
    "b = a.transpose(1, 2)  # Swaps 2nd and 3rd dimension\n",
    "b.size()\n",
    "# torch.Size([1, 3, 2, 4])\n",
    "c = a.view(1, 3, 2, 4)  # Does not change tensor layout in memory\n",
    "c.size()\n",
    "# torch.Size([1, 3, 2, 4])\n",
    "torch.equal(b, c)\n",
    "# Flase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pdnorm_model import PDNorm\n",
    "from torch import Tensor\n",
    "from dc_ldm.modules.diffusionmodules.util import conv_nd\n",
    "\n",
    "\n",
    "\n",
    "class PDGroupNorm(PDNorm, nn.GroupNorm):\n",
    "    \n",
    "    def __init__(self, num_groups: int, num_channels: int, prompt_dim: int=16, eps: float = 0.00001, \n",
    "                 affine: bool = False,  # False --> super.__init__中取消初始化self.weight和self.bias\n",
    "                 device=None, dtype=None) -> None:\n",
    "        nn.GroupNorm.__init__(self, num_groups, num_channels, eps, affine, device, dtype)\n",
    "        \n",
    "        self.mlp_scale = nn.Sequential(nn.Linear(prompt_dim, num_channels), nn.SiLU())\n",
    "        self.mlp_shift = nn.Sequential(nn.Linear(prompt_dim, num_channels), nn.SiLU())\n",
    "    \n",
    "    def forward(self, x: Tensor, prompt:Tensor) -> Tensor:\n",
    "        # 在C维，分组计算mean和var  \n",
    "        B,C,H,W = x.shape\n",
    "        x_i_standard = torch.empty(x.shape)\n",
    "        for i in range(self.num_groups):\n",
    "            x_i = x[:,int(i*(C/self.num_groups)):int((i+1)*(C/self.num_groups)),:,:] # (B,c/g,h,w)\n",
    "            mean = torch.mean(x_i, dim=1, keepdim=True)  \n",
    "            var = torch.var(x_i, dim=1, keepdim=True, unbiased=False)  # (b,1,h,w)\n",
    "            x_i_standard[:,int(i*(C/self.num_groups)):int((i+1)*(C/self.num_groups)),:,:] = \\\n",
    "                        (x_i-mean)/torch.sqrt(var+self.eps)  # (B,c/g,h,w)\n",
    "        #####################\n",
    "        scale = self.mlp_scale(prompt).reshape((B,C,1,1))  \n",
    "        shift = self.mlp_shift(prompt).reshape((B,C,1,1))  # (B,prompt_dim)-->(B,C)-->(B,C,1,1)\n",
    "        out = x_i_standard * scale + shift\n",
    "        # out = F.group_norm(\n",
    "        #     x, self.num_groups, scale, shift, self.eps)        \n",
    "        return out\n",
    "\n",
    "in_layers = nn.Sequential(\n",
    "            PDGroupNorm(32,192,16),\n",
    "            nn.SiLU(),\n",
    "            conv_nd(2, 192, 192, 3, padding=1),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer = PDGroupNorm(32, 192)\n",
    "# x:(5,192,64,64) p:(5,16) emb(5,768)\n",
    "x = torch.ones(5,192,64,64)\n",
    "p = torch.ones(5, 16)\n",
    "out0 = in_layers[0](x, p)\n",
    "print(out0.shape)\n",
    "out1 = in_layers[1](out0)\n",
    "print(out1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mind-vis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
